---
title: "Mobility & R0 Correlation Report"
author: "chfal"
date: "6/10/2020"
output: html_document
---

# Section I: Introduction

In this script, we will be correlating the R0 (the basic reproduction number of an epidemic) of coronavirus disease with the mobility reports generated by Google and Apple to evaluate to what degree social distancing was effective.

Covid-19 is a novel coronavirus discovered in Wuhan, China in late 2019. Since then it has spread to almost every continent in the world and achieved global pandemic status.

Since there is no vaccine or reliable drug to cure Covid-19, one of the ways we can reduce the net reproduction number and contain the epidemic is through social distancing.

In this project, we use the mobility reports provided by Apple and Google from users' anonymized smartphone data as a proxy for social distancing.

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
library(covid19mobility) #for getting mobility data
library(tidyverse) # for tidying everything!
library(urbnmapr) # for getting shapefiles of states and counties to map
library(sf) # for working with shapefiles
library(lubridate) # for working with dates
library(R0) #for calculating R0
library(chron) #for working with dates
library(gganimate) # for animating / saving plots
```
# Section I: Mobility Data

This section of the script is self-updating and makes the graphs of mobility data. It will refresh with new data every time it is run according to the covid19mobility package. These are the packages used for this section of the report.
To run this code, all you need is an Internet connection and the above packages installed in your R library. We're also using the urbnmapr package, which pulls shapefiles for mapping.

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#Get google data from the covid19mobility package.
google_mobility <- covid19mobility::refresh_covid19mobility_google_us_counties()

#clean google data
google_mobility <- google_mobility %>%
  group_by(state, date, data_type) %>%
  summarize_if(is.numeric, mean, na.rm=T) %>%
  pivot_wider(names_from=data_type) %>%
  filter(date>="2020-02-15")

#Get apple data from the covid19mobility package.
apple_mobility <- covid19mobility::refresh_covid19mobility_apple_subregion()

#Clean apple data
apple_mobility <- apple_mobility %>%
 filter(country=="United States") %>%
  dplyr::select(c(2,3,7,8)) %>%
  rename("state"="location") %>%
  filter(date>="2020-02-14")


#Get state shapefiles
states_sf <- get_urbn_map("states",sf=TRUE)  %>%
  rename("state"="state_name") %>%
  st_as_sf()

#Get data by every week, starting from the 15th of February. We do this by adding a "start date" column.

google_mobility$start_date = cut(google_mobility$date, breaks="7 days")
apple_mobility$start_date = cut(apple_mobility$date, breaks="7 days")

final_mobility_google <- group_by(google_mobility, state, start_date) %>% summarize_if(is.numeric, mean, na.rm=T)

final_mobility_apple <- group_by(apple_mobility, state, start_date, data_type) %>% summarize_if(is.numeric, mean, na.rm=T)
```

Working with shapefiles requires a lot of manipulation. We have a grouped data frame that we need to just convert to a regular data frame and remind R that it has shapefiles using the _st_as_sf()_ function.

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
# join with shapefiles
google_shapefiles <- as.data.frame(final_mobility_google) %>%
  left_join(states_sf) %>%
  st_as_sf()

apple_shapefiles <- as.data.frame(final_mobility_apple) %>%
  left_join(states_sf) %>%
  st_as_sf()

```

Now we are going to plot. We have a lot of things to plot because the Google data is broken down by section. This may (is) look like a lot of copy/pasting code, but I can't do it in a for loop because I wanted each one to have its own custom title, subtitle, and prefix. 

We can see that the percent increase in time spent in residences increased to 30% more than the baseline at its maximum when everyone stayed home during the pandemic.

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot residential data from google
residential <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=google_shapefiles, aes(fill=residential_percent_change_from_baseline)) +
  labs(fill="% change residential", title="date: {current_frame}", subtitle="Percent Change in Residential Movement from Google Mobility Data") +
  theme(legend.title = element_text("% change residential")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(residential)
# animate(residential, device = "png", renderer = file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "residential"))

```

However, we see a steep reduction in almost everything else.

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot grocery data from google
grocery <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=google_shapefiles, aes(fill=grocery_and_pharmacy_percent_change_from_baseline)) +
  labs(fill="% change grocery/pharmacy", title="date: {current_frame}", subtitle="Percent Change in Grocery/Pharmacy Visits from Google Mobility Data") +
  theme(legend.title = element_text("% change grocery")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(grocery)
# animate(grocery, device = "png", renderer = file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "grocery"))
```

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot park data from google
parks <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=google_shapefiles, aes(fill=parks_percent_change_from_baseline)) +
  labs(fill="% change parks", title="date: {current_frame}", subtitle="Percent Change in Park Visits from Google Mobility Data") +
  theme(legend.title = element_text("% change parks")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(parks)
# animate(parks, device = "png", renderer = file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "parks"))
```

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot transit data from google
transit <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=google_shapefiles, aes(fill=transit_stations_percent_change_from_baseline)) +
  labs(fill="% change transit", title="date: {current_frame}", subtitle="Percent Change in Transit Usage from Google Mobility Data") +
  theme(legend.title = element_text("% change transit")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(transit)
# animate(transit, device = "png", renderer = file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "transit"))

```

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot workplace visit data from google
workplaces <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=google_shapefiles, aes(fill=workplaces_percent_change_from_baseline)) +
  labs(fill="% change workplaces", title="date: {current_frame}", subtitle="Percent Change in Workplace Visits from Google Mobility Data") +
  theme(legend.title = element_text("% change workplaces")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(workplaces)

# animate(workplaces, device = "png", renderer = file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "workplaces"))
```

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot retail data from google
retail <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=google_shapefiles, aes(fill=retail_and_recreation_percent_change_from_baseline)) +
  labs(fill="% change retail", title="date: {current_frame}", subtitle="Percent Change in Retail/Recreation from Google Mobility Data") +
  theme(legend.title = element_text("% change retail")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(retail)
# animate(retail, device = "png", renderer = file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "retail"))
```

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#plot apple data
apple <- ggplot() +
  geom_sf(data=states_sf) +
  geom_sf(data=apple_shapefiles, aes(fill=value)) +
  labs(fill="% change driving", title="date: {current_frame}", subtitle="Percent Change in Driving Activity from Apple Mobility Data") +
  theme(legend.title = element_text("% change driving")) +
  scale_fill_continuous(type="viridis") +
  transition_manual(start_date)

animate(apple)
# animate(apple, device="png", renderer=file_renderer("C:/Users/chfal/Desktop/ICOMPBIO/CLEOS_RESEARCH_PJECT/" , prefix = "apple_driving"))
```

# Part II: Calculating R0

We are now going to calculate R0. We are going to use the data from Johns Hopkins broken down by time series across the USA. Unfortunately there is no package to neatly pull the data from this repository each day to self-update. Therefore, this script will be updated periodically.

Here are the additional packages used.

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
library(R0) #for calculating R0
```

We need to read in the data. I used the US Census 2019 database to obtain the population of each state. This is important because the value of R0 depends on the pouplation size (i.e. how many susceptible individuals the population has).

```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#original data from Johns Hopkins
original_data = read_csv("time_series_covid19_confirmed_US.csv")

#population data from 2019 US government database
pop_est <- read_csv("pop_est.csv") %>%
  dplyr::select(c(NAME,POPESTIMATE2019)) %>%
  rename(Province_State=NAME)

#original data, but with population appended at the end
cases_with_pop <- left_join(original_data, pop_est) %>%
  group_by(Province_State) %>%
  summarize_if(is.numeric, sum, na.rm=TRUE)

#selecting only the case counts, population data, and state to make a new matrix
case_counts_raw = cases_with_pop[,30:166]
case_counts_raw <- as.matrix(case_counts_raw)
rownames(case_counts_raw) = cases_with_pop$Province_State

# taking the difference of case counts rather than the cumulative case counts for a final matrix
case_counts <- apply(case_counts_raw,1, diff) %>%
  t()

# filters any negative values from the difference function that may have been introduced. if there's anything in case_counts that's less than zero, we're just going to set it equal to 0.
case_counts[case_counts<0]=0


#GENERATION TIME FUNCTION
mGT = generation.time("gamma", c(3, 1.5))

# FOR LOOP ----------------------------------------------------------------

# counters / setup for for-loop

#reset everything in case_counts that's less than 0 to be equal to 0 again because sometimes it gets reset

case_counts[case_counts<0]=0

# removed all cases where you didn't know what the population was
case_counts <- case_counts[case_counts[,136] !=0,]

weekly <- seq(0,ncol(case_counts), by=7)

i=1
j=1
k=1
l=1

r0 <- data.frame(State=character(), Week=character(),R0=double())


# implement case count to be lower
for (i in 1:nrow(case_counts)){ # we are going to iterate over all the states
  for (j in 1:19){ #add cases to be cumulative across the week
    x=case_counts[i,(weekly[j]+1):weekly[j+1]]
    if (sum(x)<=10) { #cutoff is now less than 10 cases in a week
      R0=NA
    }
    else{
      R0EG = estimate.R(case_counts[i,], GT=mGT,  methods=c('EG'), pop.size = case_counts[,138], nsim=10, begin=weekly[j]+1, end=weekly[j+1])
      R0 = R0EG$estimates$EG$R
    }
    r0[l,1]=rownames(case_counts)[i]
    r0[l,2]=j
    r0[l,3]=R0
    l=l+1
  }
}

r0 <- r0 %>%
  tibble()

#cut the result to quantiles - remove anything in the 95th percentile and the 5th percentile
r0$R0[r0$R0>quantile(r0$R0, .95, na.rm=T)] <- NA
r0$R0[r0$R0<quantile(r0$R0, .05, na.rm=T)] <- NA

#add labels to each week's start date
Week <- as.character(c(1:19))
dates <- colnames(case_counts[,c(1,8,15,22,29,36,43,50,57,64,71,78,85,92,99,106,113,120, 127)])

positions <- data.frame(Week, dates)

#write csv to save it
r0 <- left_join(r0, positions)
write_csv(r0, "result_data_frame.csv")
```

#Correlating R0 with Mobility Data

Now we are going to correlate our R0 scores by week with our mobility scores by week for Apple and Google. We are going to do this using the linear modelling function of R. Even though the distributions of the mobility scores, as well as the R0, are not normal (and therefore won't be perfectly straight), we can still use this method for simple correlations.
```{r, error=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
# this data is the result data frame from the above for loop, but read directly in for convenience because the for loop takes some time to run. 
r0 <- read_csv("result_data_frame.csv") %>%
  mutate(date=as.Date(dates, "%m/%d/%y")) %>%
  dplyr::select(c("State","R0","date"))

final_mobility_apple <-final_mobility_apple %>%
  mutate(start_date=as.Date(start_date)) %>%
  dplyr::select(c("start_date","data_type","value"))

final_mobility_google <-final_mobility_google %>%
  mutate(start_date=as.Date(start_date))

corr_apple <- left_join(final_mobility_apple, r0, by =c("start_date"="date", "state"="State"))

corr_google <- right_join(final_mobility_google,r0, by=c("start_date"="date","state"="State"))
```

Now we are going to add the weather data!

```{r}
weather <- read_csv("aggregated_data_156_timepoints.csv")

weather <- weather %>%
  pivot_longer(c(13:168),names_to="date",values_to="temp_in_K")

weather <- weather %>%
  mutate(date=ymd(weather$date))

weather <- weather %>%
  filter(date>="2020-02-15")

weather$start_date =cut(weather$date, breaks="7 days")

weather <- weather %>%
  group_by(Province_State, start_date) %>%
  summarize_if(is.numeric, mean, na.rm=T)

weather$start_date <- as.Date(weather$start_date, "%Y-%m-%d")

weather_total <- weather %>% dplyr::select("Province_State", "start_date", "temp_in_K")
```


Now we are going to make the final data set for correlating.
```{r}
final_google_corr <- left_join(corr_google, weather_total, by=c("state"="Province_State", "start_date"="start_date"))
write_csv(final_google_corr, "final_google_corr.csv")

final_apple_corr <- left_join(corr_apple, weather_total, by=c("state"="Province_State", "start_date"="start_date"))
write_csv(final_apple_corr, "final_apple_corr.csv")

```


